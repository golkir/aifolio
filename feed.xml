<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://golkir.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://golkir.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-25T15:24:21+00:00</updated><id>https://golkir.github.io/feed.xml</id><title type="html">blank</title><subtitle>Kirill Goltsman - software engineer and tech writer, AI/ML, statistics, blockchain. </subtitle><entry><title type="html">Qbox vs Self-Hosted Elasticsearch</title><link href="https://golkir.github.io/blog/2022/qbox-vs-self-hosting/" rel="alternate" type="text/html" title="Qbox vs Self-Hosted Elasticsearch"/><published>2022-04-24T15:12:00+00:00</published><updated>2022-04-24T15:12:00+00:00</updated><id>https://golkir.github.io/blog/2022/qbox-vs-self-hosting</id><content type="html" xml:base="https://golkir.github.io/blog/2022/qbox-vs-self-hosting/"><![CDATA[<p>As developers or application owners, we are often faced with a complex decision which one to choose - a managed Elasticsearch provided by professional Elasticsearch-as-a-service companies like Qbox, or our own on-premises or cloud-hosted Elasticsearch clusters using manual installation, configuration, and maintenance. The final decision might depend on numerous factors - cost, upfront or incremental investment in the in-house computer infrastructure, the level of in-house expertise and cost of support, ease of maintenance, scalability concerns, and many more. Thus, the choice between self-hosted and managed Elasticsearch may involve multiple trade-offs which are hard to recognize. In this article, we’ll compare these two alternatives: Qbox-hosted and self-hosted Elasticsearch making you aware of salient pros and cons of these two options.</p> <p>But before embarking on the comparison, let us spell out basic definitions.</p> <h4 id="what-is-self-hosted-elasticsearch">What is Self-Hosted Elasticsearch?</h4> <p>Basically, a self-hosted Elasticsearch may involve two options:</p> <ul> <li>you are provisioning Elasticsearch on-premises using in-house computer infrastructure and expertise. Your DevOps engineers install, configure, maintain, upgrade and trouble-shoot Elasticsearch cluster/s operated by your company.</li> <li>you are running a self-hosted Elasticsearch on the infrastructure of a Cloud Service Provider (CSP) like Amazon AWS of Google Cloud Platform (GCP). In this case, you also do manual installation and configuration of an Elasticsearch cluster although now using the cloud-based Infrastructure-as-a-Service (IaaS) instead of your in-house resources to provision and scale Elasticsearch clusters. Note that this approach is not the same as using CSP-managed Elasticsearch solutions like Amazon Elasticsearch Service that also make it easy to deploy, secure, operate, and scale Elasticsearch.</li> </ul> <h4 id="what-is-qbox-hosted-elasticsearch">What is Qbox-Hosted Elasticsearch?</h4> <p>With Qbox, you are hosting Elasticsearch remotely having access to the UI and services provided by our company.</p> <ul> <li>Qbox automatically installs, provisions, scales, and resizes your Elasticsearch cluster/s on remote CSP infrastructure provided by AWS, Rackspace or SoftLayer.</li> <li>Qbox manages Elasticsearch configuration, monitoring, alerting and provides 24/7 support for your cluster operations.</li> <li>You don’t need to interact with CSPs to configure and scale your Elasticsearch. Most things that you need are provided out-of-the-box by Qbox.</li> </ul> <p>Now, as you have a better understanding of available options, let’s compare them in more detail.</p> <h3 id="installation">Installation</h3> <h4 id="qbox"><strong>Qbox</strong></h4> <p>Elasticsearch installation with Qbox is plain and simple. You don’t need to download and install any software packages: Qbox will ship the requested Elasticsearch version for you and walk you through a simple installation process at the end of which you’ll have a fully operational Elasticsearch cluster on the infrastructure of a CSP you prefer. Qbox currently supports three major CSPs including Amazon AWS, Rackspace, and SoftLayer.</p> <p>After you’ve selected a CSP, Qbox will automatically manage the provisioning of virtual machines and storage for the ES cluster freeing you from the tedious configuration work and the need to create and manage your own accounts on the CSP. During the installation process, Qbox will also let you specify the number of nodes to create and computing resources (RAM and processors) to service them. For example, in the image below, we see the Rackspace cluster configuration that offers a wide range of infrastructure option ranges, which allows deploying the exact amount of resources to match your needs.</p> <p><img src="Image1_new-cluster-rackspace.png" alt=""/></p> <p>During the installation process, you can also select the region in which to deploy your ES cluster, Elasticsearch version, and the monitor plug-in. The entire installation process should not take more than 2 minutes if you are using the Qbox-hosted Elasticsearch.</p> <h4 id="self-hosted-elasticsearch">Self-hosted Elasticsearch</h4> <p>Needless to say, that installing self-hosted Elasticsearch would require much more manual work including finding an Elasticsearch version compatible with your servers, configuring security, linking plug-ins you would like to use with your Elasticsearch, and many more. Another thing is that Elasticsearch installation details differ across operating systems (e.g Windows and Linux). You’ll need to spend additional time figuring out the installation process for your platform. Your OS environment should be also prepared. In particular, since Elasticsearch is built using Java, it requires Java environment (at least Java 8). All these major and small prerequisites will make your team spend more time preparing and configuring servers and software to work with Elasticsearch.</p> <h3 id="configuration"><strong>Configuration</strong></h3> <h4 id="qbox-1"><strong>Qbox</strong></h4> <p>Elasticsearch has good defaults and requires little configuration. However, Qbox abstracts a configuration process even further and simplifies it using an easy-to-use web-based UI. With Qbox-hosted Elasticsearch, you have the most important configuration options available:</p> <ul> <li>setting the cluster’s name and selecting the cluster’s region</li> <li>changing the default number of replicas and shards for your indexes (AWS allows configuring replicas, Softlayer and Rackspace support both)</li> <li>configuring traffic rules and security</li> <li>choosing plug-ins to install with your Elasticsearch cluster. Qbox supports a wide variety of ES plug-ins like <a href="https://github.com/elastic/elasticsearch-migration">Elasticsearch Migration</a> , <a href="https://github.com/graphaware/graph-aided-search">Graphaware Graph-Aided Search</a>, <a href="https://github.com/elasticsearch/elasticsearch-analysis-phonetic">Phonetic Analysis</a>, <a href="https://github.com/yakaz/elasticsearch-analysis-combo">Combo Analysis</a> and many more.</li> <li>enabling alerting, monitoring, and Kibana</li> </ul> <p><strong>Self-Hosted Elasticsearch</strong></p> <p>To configure a self-hosted ES cluster, you may need to work with three files which are <code class="language-plaintext highlighter-rouge">elasticsearch.yml</code> for configuring Elasticsearch, <code class="language-plaintext highlighter-rouge">jvm.options</code> for configuring Elasticsearch JVM settings, and <code class="language-plaintext highlighter-rouge">log4j2.properties</code> for configuring Elasticsearch logging. Manual configuration can be an error-prone work because you have to enter information by hand and always consult the official documentation to make sure you’re doing everything right.</p> <p>To illustrate the difference between Qbox and self-hosted ES configuration, let’s take a look at setting a cluster’s name. All nodes should know this name to join the cluster. As we’ve seen, in Qbox, you choose the cluster’s name during the installation process. In contrast, a self-hosted ES is created with a default cluster name (“Elasticsearch”) which should be changed in the configuration file after installation like this:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">cluster.name</span><span class="pi">:</span> <span class="s">YOUR CLUSTER NAME</span>
</code></pre></div></div> <p>Guess what’s easier! The answer is obvious: using Qbox UI to configure your cluster without tinkering with configuration files is much easier.</p> <p>The same is true of plug-ins. To install a plug-in using a self-hosted ES, you should use the plug-in manager. For example, to install the Phonetic Analysis plug-in, you need to open your terminal and enter the following command:</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>bin/elasticsearch-plugin <span class="nb">install </span>analysis-phonetic
</code></pre></div></div> <p>That’s easy but what if you need a dozen of different plug-ins which also have to be configured before using them? Also, to work smoothly, plug-ins should be installed on every node in the cluster, and each node should be restarted afterwards. Needless to say, Qbox will take care of this. After you’ve edited the list of required plug-ins on Qbox, the system will automatically restart each node and re-provision the cluster for you.</p> <p>Also, with the self-hosted Elasticsearch, you may need to configure the OS to allow Elasticsearch process to use more system resources than usually allocated by default. That is because, ideally, Elasticsearch should run alone on the server and have access to all resources available to it.</p> <p>Finally, Qbox makes it easy for your cluster to go into production since it adjusts the infrastructure and system environments to work at scale. When using self-hosted Elasticsearch, however, you should ensure that the following things are done before going into production:</p> <ul> <li>disable swapping</li> <li>increase file descriptors</li> <li>ensure that sufficient threads are available</li> <li>ensure that sufficient virtual memory is available</li> <li>ensure proper JVM DNS cache settings</li> </ul> <p>All this makes going into production a challenging task that requires a lot of planning, time, and expertise to be implemented.</p> <h3 id="scalability">Scalability</h3> <h4 id="qbox-2"><strong>Qbox</strong></h4> <p>Scalability is one of the major requirements of the modern distributed applications which is hard to meet using a self-hosted solution. If your unable to cope with the sudden surge of your ES server’s load or memory shortage by scaling the underlying infrastructure, you’ll probably fail to retain your customers. Qbox solves this problem by enabling automatic resizing of your clusters using AWS, Rackspace or SoftLayer APIs under the hood. To resize your cluster, you don’t have to access remote CSP accounts and manually launch and configure new VM instances. Qbox will just allow you to select the size of RAM and a number of replicas, and then automatically adjust the right CPU configuration for your cluster. You’ll be able to see how your costs change as well. It can’t be easier than that!</p> <p><img src="Image2_resize-cluster-aws.png" alt=""/></p> <h4 id="self-hosted-elasticsearch-1"><strong>Self-Hosted Elasticsearch</strong></h4> <p>Scaling self-hosted Elasticsearch is a complex task that requires a lot of planning and upfront investment in computer infrastructure (e.g servers, memory, network). If your cluster has run out of memory and storage and the company has no additional servers to allocate to your cluster, you’ll need to buy some more. That process takes time and costs you money. After you’ve obtained new resources, your IT department will need to manually configure and deploy new servers, OS, configure the network, security, and environmental variables for your Elasticsearch cluster. Even if you’re using some IaaS provider for hosting the Elasticsearch cluster, you’ll still need to manually deploy and configure new VMs for it.</p> <h3 id="backups-cloning-and-disaster-recovery">Backups, Cloning and Disaster Recovery</h3> <h4 id="qbox-3"><strong>Qbox</strong></h4> <p>Qbox handles the backup and recovery process for your Elasticsearch cluster automatically. Backups run daily using Elasticsearch’s Snapshot API under the hood. Qbox stores these backups on the highly available remote storage during 7 days so you always have access to the recent snapshots of your cluster. Restoring a cluster from the prior backup is plain and simple: just select a backup snapshot you want with ‘<strong>View backups</strong>’ under the “<strong>Manage</strong>” drop-down in your Qbox dashboard and Qbox will re-provision a new cluster using the selected snapshot.</p> <p><img src="Image3_qbox-backups.png" alt=""/></p> <p>This simple backup restoration process via the dashboard allows Qbox customers to quickly take action when issues arise.</p> <p>In addition to backups, Qbox supports cloning of existing clusters. Using “<strong>Clone from backup</strong>” feature under the “<strong>Manage</strong>” drop-down, you can instantly create a new cluster with the desired setup and configuration. This feature is especially useful when the failure of your cluster is imminent or the cluster is unrecoverable which might happen due to complete resource overload or cloud outage.</p> <p><img src="Image4_cloning-cluster.png" alt=""/></p> <h4 id="self-hosted-elasticsearch-2"><strong>Self-Hosted Elasticsearch</strong></h4> <p>To create backups with a self-hosted Elasticsearch, you need to use Snapshot and Restore modules manually or develop some sort of back-end service or cron job if you want to create backups and clones automatically. Overall, creating a backup in a self-hosted Elasticsearch involves a lot of requirements:</p> <ul> <li>in order to clone a multi-node cluster on-premises, you’ll need to use a Network File System (NFS) share and make it accessible to all nodes on the same mounting point.</li> <li>after NFS is installed, you’ll need to register a repository to save backup snapshots to.</li> <li>grant Elasticsearch permissions to write to a new repository</li> <li>specify a path to the repository in the ES configuration file</li> <li>making a cluster’s backup snapshot and testing it</li> </ul> <p>As you see, backup management in the self-hosted Elasticsearch involves quite a lot of steps. At each step, you should ensure that everything is done correctly because creating snapshots of your data is not something to take lightly. A backup that can’t restore your Elasticsearch cluster at a critical time is worth nothing. Therefore, you should think twice before doing Elasticsearch backups manually. Qbox dramatically simplifies this process ensuring that your backups are always available and properly tested to be restored at the most critical time.</p> <h3 id="kibana">Kibana</h3> <p>Kibana is an analytics and visualization system designed to work with Elasticsearch. It can be used to search, view and manage your Elasticsearch indices. Kibana allows making sense of huge volumes of data using its simple, browser-based interface.</p> <h4 id="qbox-4"><strong>Qbox</strong></h4> <p>Installing Kibana with Qbox is as simple as checking the corresponding box during ES installation process. Qbox will ensure the compatibility of Elasticsearch and Kibana versions, create appropriate credentials for communication between Kibana and Elasticsearch instances, and configure Kibana to access ES by assigning ES host and port. After your Elasticsearch is provisioned, you can access Kibana by clicking the link that may be found in your Qbox dashboard. It’s as easy as that!</p> <p><img src="Image5_Kibana-UI.png" alt=""/></p> <h4 id="self-hosted-elasticsearch-3"><strong>Self-Hosted Elasticsearch</strong></h4> <p>When using a self-hosted Elasticsearch, you’ll need to install Kibana manually. It’s quite simple but takes extra time. In particular, you should ensure that Kibana and Elasticsearch installations have the same versions. Moreover, once Kibana is installed, you should configure Elasticsearch host and port numbers, username and password for Kibana to access your cluster. These settings need to be edited in your <code class="language-plaintext highlighter-rouge">kibana.yml</code> configuration file. At the bare minimum, you’ll need to configure <code class="language-plaintext highlighter-rouge">elasticsearch.url</code> , <code class="language-plaintext highlighter-rouge">elasticsearch.username</code> and <code class="language-plaintext highlighter-rouge">elasticsearch.password:</code> in Kibana configuration file. In sum, installing Kibana manually involves quite a lot of steps compared to the automatic installation of Kibana by Qbox.</p> <h3 id="security">Security</h3> <p>In its basic form, Elasticsearch security settings should ensure the following:</p> <ul> <li>prevent unauthorized access to your cluster through password protection,</li> <li>enabling IP filtering and role-based access control (RBAC)</li> <li>preserving data integrity with SSL/TLS encryption</li> <li>maintaining an audit trail</li> </ul> <p>Let’ see how Qbox and self-hosted Elasticsearch approach these and other security requirements.</p> <h4 id="qbox-5"><strong>Qbox</strong></h4> <p>Qbox-hosted Elasticsearch is provisioned with the basic security settings which prevent unauthorized access and ensure data integrity of your cluster. In addition to SSL, all Elasticsearch clusters are provisioned with HTTP Basic authentication that includes username and password. Qbox also provides whitelisting for HTTP and transport traffic, VPC peering, and encryption-at-rest. Qbox currently does not allow custom SSL certificates on the clusters because they are not viable in the long run.</p> <h4 id="self-hosted-elasticsearch-4"><strong>Self-Hosted Elasticsearch</strong></h4> <p>To secure your self-hosted Elasticsearch, you’ll need to install X-Pack on every node of your cluster. X-Pack is an official Elasticsearch package that supports security, monitoring, alerting, analytics, and more. Although the product comes with a lot of free features, full access to all functionality of the Security (former Shield) module including encrypted communication, RBAC, LDAP and Active Directory authentication, SAML authentication, Audit logging, and more <a href="https://www.elastic.co/subscriptions#request-info">requires a Platinum subscription</a>. In addition to this, X-Pack security module requires a lot of manual configuration that can be only done by qualified computer security professionals.</p> <h3 id="free-alerting-and-monitoring">Free Alerting and Monitoring</h3> <p>Think of the use cases for alerting and monitoring in your Elasticsearch application: CPU usage is suddenly surging, response errors are spiking, traffic is decreasing unexpectedly. All these issues tell much about the health and performance of your Elasticsearch cluster and you need to have monitoring and alerting enabled to know about these issues as they arise.</p> <h4 id="qbox-6">Qbox</h4> <p>Qbox-hosted ES ships with <a href="https://github.com/Yelp/elastalert">ElastAlert</a>, a simple alerting framework that helps identify anomalies, spikes and other unusual patterns in your Elasticsearch data. You can enable alerting during or after installation of your ES cluster on Qbox. The system will let you define alerting rules in the YAML format to choose scenarios and events when alerts should be sent. In addition, by default, Qbox supports email alerting using Qbox’s SMTP host.</p> <p><img src="Image6_ElasAlert.png" alt=""/></p> <p>With Qbox-hosted ElastAlert, you can design a customized alerting system that can be used for the following:</p> <ul> <li>Link ES alerts to Kibana dashboards</li> <li>Create periodic reports based on alerts data</li> <li>Organize your alerts in classes using a unique key field</li> <li>Intercept and improve match data</li> </ul> <p>At the moment, ElastAlert supports a variety of alert types including Email, JIRA, HipChat, Slack, Telegram, SNS, and more.</p> <p>Also, by default, Qbox supports a number of free monitoring plug-ins like <a href="https://github.com/mobz/elasticsearch-head">Elasticsearch Head</a>, <a href="https://github.com/lmenezes/elasticsearch-kopf">Elasticsearch Kopf</a>, and <a href="https://github.com/ElasticHQ/elasticsearch-HQ">Elasticsearch HQ</a> among others. In addition to these plug-ins, Qbox regularly checks clusters for potential issues and notifies users if any problems arise.</p> <h4 id="self-hosted-elasticsearch-5">Self-Hosted Elasticsearch</h4> <p>With self-hosted Elasticsearch, you need to select, install and configure alerting and monitoring plug-ins manually. You should also configure the alerting transport such as your email service or Slack channel. ElastAlert is a great choice for these purposes but your team will have to spend extra time to integrate it. Other great alerting options for self-hosted ES include X-Pack that supports a variety of alerting types, integration with your monitoring infrastructures, and more. However, X-Pack alerting (via Watcher) requires a Gold subscription.</p> <h3 id="support"><strong>Support</strong></h3> <p>Support is critical when something unexpected happens. And it always happens in complex environments like Elasticsearch clusters. Having a professional support at your fingertips to instantly address the issues is one of the major requirements for successfully running your Elasticsearch cluster.</p> <h4 id="qbox-7"><strong>Qbox</strong></h4> <p>At Qbox, free support is a part of the support response <a href="https://qbox.io/support">Service Level Agreement (SLA)</a>. It covers one hour a day for production issues, four hours for product questions, eight hours for updates, migrations and maintenance, and 24 hours for general issues. Qbox’s dedicated support includes certified database and ES professionals who have a deep knowledge of this technology and resources to address dozens of reasons why your ES node might be unresponsive. Having a dedicated support at your fingertips is even more important if you’re running Elasticsearch in production since you might not have enough time and expertise to fix emerging issues.</p> <h4 id="self-hosted-elasticsearch-6"><strong>Self-Hosted Elasticsearch</strong></h4> <p>To run Elasticsearch on-premises, your company needs to hire DevOps specialists for server maintenance, management, troubleshooting and making upgrades. A qualified specialist/s should be always available to fix issues are they arise. Needless to say, hiring such specialists introduces additional costs. Also, many companies have a misconception about Elasticsearch support services offered by major CSPs. Many people expect that when self-hosting Elasticsearch on the CSP infrastructure (e.g Amazon AWS) and the cluster is non-responsive for some reason, they can reach support to fix the issue. The problem is that, however, issues in your Elasticsearch are frequently not at the node level while the CSP’s responsibility for you is only to ensure that your VMs are up. Remember that Elasticsearch is an open-source technology created by the community and having a huge code base. You should not expect CSPs to have the same level of expertise in Elasticsearch as in their proprietary products.</p> <h3 id="summary">Summary</h3> <p>We’re almost there as we’ve covered key differences between Qbox and self-hosted Elasticsearch. Now, hopefully, you have a better understanding of what both solutions bring to the table. Let’s briefly sum up their key benefits and pitfalls.</p> <h4 id="qbox-benefits">Qbox Benefits</h4> <ul> <li>Easy Elasticsearch installation, configuration, and management using a simple web-based UI</li> <li>dedicated provisioning of computer resources for your cluster</li> <li>one-click scaling and resizing of your Elasticsearch cluster in a fraction of time without query interruption</li> <li>immediate and continuous disaster recovery with Qbox’s high-availability storage system mirrored in the cloud</li> <li>built-in security and monitoring of your cluster’s health.</li> <li>free alerting and other useful plug-ins out-of-the-box</li> <li>an efficient support response Service Level Agreement (SLA)</li> </ul> <p>**Self-Hosted ES Benefits **</p> <ul> <li>More fine-grained control over configuration. You can have a better control over how ES works with a full access to ES and Kibana configuration files and Elasticsearch API.</li> <li>You can add proprietary products like X-Pack not supported by Qbox to your Elasticsearch installation. With an option to select any plug-ins and services not supported by Qbox, you can design any customized Elasticsearch stack that fits your needs.</li> </ul> <p>Hosting Elasticsearch on-premises and having more fine-grained control over configuration, however, comes with certain limitations:</p> <ul> <li>Requires substantial upfront investment in computer infrastructure and software</li> <li>Necessitates in-house expertise or paid third-party support for server maintenance and management of updates and backups.</li> <li>Scaling self-hosted Elasticsearch might take more time due to the manual deployment of new servers and configuring them.</li> </ul> <h3 id="conclusion"><strong>Conclusion</strong></h3> <p>Self-hosted Elasticsearch is definitely an option if you want a highly customized ES solution and have enough in-house expertise and resources to support maintenance, recovery management, scaling, and upgrading of your Elasticsearch clusters. Hosting ES on-premises, however, will inevitably result in more planning and upfront investment and may turn out to be unsustainable in the long run if capacities are overestimated. The lesson learned by many companies who opted for self-hosted Elasticsearch is that if you can find an expert offering a service you need - you better use them. Although it might be cheaper in the short term to manage Elasticsearch yourself, managing it at scale might end up in a lot of bottlenecks, maintenance issues, bugs all of which means interruptions of your service. There are simply dozens of things that one need to consider when using Elasticsearch in production. These issues can be professionally addressed by someone who specializes in providing Elasticsearch-as-a-service as its major business. Qbox experts will take the pain of building and managing your Elasticsearch cluster away from you which will ultimately free up time to concentrate on building excellent products letting others take care of the rest.</p>]]></content><author><name></name></author><category term="data"/><category term="elasticsearch"/><category term="data"/><summary type="html"><![CDATA[As developers or application owners, we are often faced with a complex decision which one to choose - a managed Elasticsearch provided by professional Elasticsearch-as-a-service companies like Qbox, or our own on-premises or cloud-hosted Elasticsearch clusters using manual installation, configuration, and maintenance. The final decision might depend on numerous factors - cost, upfront or incremental investment in the in-house computer infrastructure, the level of in-house expertise and cost of support, ease of maintenance, scalability concerns, and many more. Thus, the choice between self-hosted and managed Elasticsearch may involve multiple trade-offs which are hard to recognize. In this article, we’ll compare these two alternatives: Qbox-hosted and self-hosted Elasticsearch making you aware of salient pros and cons of these two options.]]></summary></entry><entry><title type="html">How to ship MySQL Logs to Elasticsearch with Filebeat</title><link href="https://golkir.github.io/blog/2022/mysql_logs_filebeat_Esearch/" rel="alternate" type="text/html" title="How to ship MySQL Logs to Elasticsearch with Filebeat"/><published>2022-04-23T15:12:00+00:00</published><updated>2022-04-23T15:12:00+00:00</updated><id>https://golkir.github.io/blog/2022/mysql_logs_filebeat_Esearch</id><content type="html" xml:base="https://golkir.github.io/blog/2022/mysql_logs_filebeat_Esearch/"><![CDATA[<p>Effective log management involves a possibility to instantly draw useful insights from millions of log entries, identify issues as they arise, and visualize/communicate patterns that emerge out of your application logs. Fortunately, ELK stack (Elasticsearch, Logstash, and Kibana) makes it easy to ship logs from your application to ES collections for storage and analysis. Recently, Elastic infrastructure was extended by useful tools for shipping logs called Beats. Filebeat is a part of Beats tool set that can be configured to send log events either to Logstash (and from there to Elasticsearch), or even directly to the Elasticsearch. The tool turns your logs into searchable and filterable ES documents with fields and properties that can be easily visualized and analyzed.</p> <p>In the previous <a href="https://qbox.io/blog/how-to-ship-linux-system-logs-elasticsearch-filebeat">post</a>, we have already discussed how to use Filebeat to ship Linux system logs. Now, it’s time to show how to ship logs from your MySQL database via Filebeat transport to your Elasticsearch cluster. Making MySQL general and slow logs accessible via Kibana and Logstash will radically improve your database management, log analysis and pattern discovery leveraging the full potential of ELK stack.</p> <h3 id="tutorial">Tutorial</h3> <p>To complete the tutorial, we are using:</p> <ol> <li> <p>An Ubuntu 16.04 LTS distribution.</p> </li> <li> <p>A Qbox hosted Elasticsearch Cluster with a Kibana support.</p> <p>Qbox.io Elasticsearch clusters outperform self-hosted ES solutions thanks to the platform’s easy scalability, 24/7 uptime, easy integration with Logstash and Kibana, and free support.</p> <p>For this post, we will be using hosted Elasticsearch on Qbox.io. You can sign up or <a href="https://qbox.io/signup?utm_source=blog&amp;utm_campaign=tutorial&amp;utm_term=launch_your_cluster&amp;utm_medium=article">launch your cluster here</a>, or click “Get Started” in the header navigation. If you need help setting up, refer to <a href="https://qbox.io/blog/provisioning-a-qbox-elasticsearch-cluster?utm_source=tutorial&amp;utm_term=provision&amp;utm_medium=article&amp;utm_campaign=index-attachments-files-elasticsearch-mapper">Provisioning a Qbox Elasticsearch Cluster</a></p> </li> <li> <p>MySQL 5.7</p> </li> <li> <p>Filebeat 5.6.2</p> </li> </ol> <h3 id="installing-and-configuring-mysql"><strong>Installing and Configuring MySQL</strong></h3> <p>If you haven’t installed MySQL yet, you can use these steps to install and configure it. The first thing you need to do is to update your system.</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get update
</code></pre></div></div> <p>And then install MySQL like this:</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>mysql-server
</code></pre></div></div> <p>During the installation, you will be prompted to set your root password. Make note of it because it will be required to manage your MySQL database.</p> <p>The next step is configuring MySQL to write general and slow query log files since these configurations are disabled by default. To change configurations, you will need to edit <code class="language-plaintext highlighter-rouge">my.cnf</code> file that contains user database settings.</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>nano /etc/mysql/my.cnf
</code></pre></div></div> <p>A working configuration for general and slow queries should look like this:</p> <pre><code class="language-assembly">general_log      = 1
general_log_file = /var/log/mysql/mysql.log
slow_query_log   = 1
slow_query_log_file = /var/log/mysql/mysql-slow.log
long_query_time = 1
log_queries_not_using_indexes = 1
</code></pre> <p>Please, note that in MySQL versions &lt; 5.1.29 the variable <code class="language-plaintext highlighter-rouge">log_slow_queries</code> was used instead of <code class="language-plaintext highlighter-rouge">slow_query_log</code>.</p> <p>Make sure to restart MySQL after making these changes:</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>service mysql restart
</code></pre></div></div> <p>Now, your MySQL is ready for writing slow queries that will be shipped to your Elasticsearch cluster via Filebeat.</p> <h3 id="installing-and-configuring-filebeat">Installing and Configuring Filebeat**</h3> <p>The next step is installing Filebeat on our Ubuntu 16.04 machine. For this tutorial, we are using the latest version of Filebeat (5.6.2) released on September 26, 2017. Once Filebeat is installed, it will be working as an agent that is sending all MySQL logs to Elasticsearch or Logstash for storage and processing. You can install Filebeat from RPM, DEB package, or source. The latest version of Filebeat <a href="https://www.elastic.co/downloads/beats/filebeat">can be downloaded here.</a></p> <p>After we have installed Filebeat, we need to configure it to work with MySQL logs in our system. Similarly to other software on Linux, the default configuration of Filebeat is stored inside <code class="language-plaintext highlighter-rouge">/etc/filebeat</code> directory. There you will find <code class="language-plaintext highlighter-rouge">filebeat.yaml</code> configuration file with some examples of Filebeat configurations. Backup this file in case if something goes wrong and create a new <code class="language-plaintext highlighter-rouge">filebeat.yaml</code> file for our MySQL configuration.</p> <p>The initial configuration file for Filebeat will have the following content:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">filebeat.prospectors</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">input_type</span><span class="pi">:</span> <span class="s">log</span>
  <span class="na">paths</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">/var/log/mysql/*.log</span>
  <span class="na">registry</span><span class="pi">:</span> <span class="s">/var/lib/filebeat/registry</span>
<span class="na">output.elasticsearch</span><span class="pi">:</span>
        <span class="na">hosts</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">YOUR</span><span class="nv"> </span><span class="s">ELASTICSEARCH</span><span class="nv"> </span><span class="s">HOST"</span><span class="pi">]</span>
        <span class="na">protocol</span><span class="pi">:</span> <span class="s2">"</span><span class="s">https"</span>
        <span class="na">username</span><span class="pi">:</span> <span class="s2">"</span><span class="s">YOUR</span><span class="nv"> </span><span class="s">USERNAME"</span>
        <span class="na">password</span><span class="pi">:</span> <span class="s2">"</span><span class="s">YOUR</span><span class="nv"> </span><span class="s">PASSWORD"</span>
</code></pre></div></div> <p>After editing the configuration file, don’t forget to restart Filebeat</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>service filebeat restart
</code></pre></div></div> <p>The above configuration settings do several things:</p> <ol> <li>Filebeat prospectors define the type of logs shipped and their location.</li> <li>Output tells the Filebeat the location where log messages should be sent (i.e Qbox hosted Elasticsearch cluster in our case)</li> </ol> <p>Also, as you can see, Filebeat supports wildcard entries for path settings. So, instead of specifying all log files to be watched, we just define a wildcard location <code class="language-plaintext highlighter-rouge">/var/log/mysql/*.log</code> which tells Filebeat to watch all files in the MySQL log directory.</p> <p>In the configuration file, we have also specified the location of <strong>Filebeat registry file</strong>. The registry file is very important because it stores the state and location information that Filebeat uses to track upcoming logs. This file prevents Filebeat from sending the same entries all over again. Note the registry setting in our example configuration file above, <code class="language-plaintext highlighter-rouge">registry: /var/lib/filebeat/registry</code>.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="dl">"</span><span class="s2">source</span><span class="dl">"</span><span class="p">:</span><span class="dl">"</span><span class="s2">/var/log/mysql/mysql-slow.log</span><span class="dl">"</span><span class="p">,</span><span class="dl">"</span><span class="s2">offset</span><span class="dl">"</span><span class="p">:</span><span class="mi">364</span><span class="p">,</span><span class="dl">"</span><span class="s2">FileStateOS</span><span class="dl">"</span><span class="p">:{</span><span class="dl">"</span><span class="s2">inode</span><span class="dl">"</span><span class="p">:</span><span class="mi">1454517</span><span class="p">,</span><span class="dl">"</span><span class="s2">device</span><span class="dl">"</span><span class="p">:</span><span class="mi">2049</span><span class="p">},</span><span class="dl">"</span><span class="s2">timestamp</span><span class="dl">"</span><span class="p">:</span><span class="dl">"</span><span class="s2">2017-09-27T17:43:06.877125109+03:00</span><span class="dl">"</span><span class="p">,</span><span class="dl">"</span><span class="s2">ttl</span><span class="dl">"</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">},{</span><span class="dl">"</span><span class="s2">source</span><span class="dl">"</span><span class="p">:</span><span class="dl">"</span><span class="s2">/var/log/mysql/mysql.log</span><span class="dl">"</span><span class="p">,</span><span class="dl">"</span><span class="s2">offset</span><span class="dl">"</span><span class="p">:</span><span class="mi">5406</span><span class="p">,</span><span class="dl">"</span><span class="s2">FileStateOS</span><span class="dl">"</span><span class="p">:{</span><span class="dl">"</span><span class="s2">inode</span><span class="dl">"</span><span class="p">:</span><span class="mi">1455085</span><span class="p">,</span><span class="dl">"</span><span class="s2">device</span><span class="dl">"</span><span class="p">:</span><span class="mi">2049</span><span class="p">},</span><span class="dl">"</span><span class="s2">timestamp</span><span class="dl">"</span><span class="p">:</span><span class="dl">"</span><span class="s2">2017-09-27T17:45:47.828072524+03:00</span><span class="dl">"</span><span class="p">,</span><span class="dl">"</span><span class="s2">ttl</span><span class="dl">"</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">}]</span>
</code></pre></div></div> <p>As you see, the registry file contains information about MySQL logs location, disc on which they are stored and the <strong>inode</strong> number of the file.</p> <p>Once we have specified all required Filebeat settings mentioned above and restarted the Filebeat, it should begin sending information to our Elasticsearch cluster. To check whether Filebeat actually created new indices in our Elasticsearch, we can use Kibana.</p> <h3 id="kibana"><strong>Kibana</strong></h3> <p><img src="" alt="Image1"/></p> <p>To find our MySQL logs in Elasticsearch, we first need to create an index pattern in Kibana management tab. The pattern for Filebeat logs is <strong>filebeat-</strong>*. After saving the pattern, Kibana will show the list of your MySQL logs on the dashboard:</p> <p><img src="" alt="Image2 displaying log entries in Kibana dashboard"/></p> <p>As you can see, Filebeat transforms MySQL logs into <em>objects</em> that hold specific properties of logs such as timestamps, source file, log message, id and some others. This makes it easy to search and filter your MySQL logs using Elasticsearch queries. This approach also enables you to easily create visualizations in Kibana or fuel logs into AI/ML models and algorithms for pattern recognition and anomaly detection. To leverage this functionality, you can create custom fields that will track additional information from MySQL logs as per your requirements.</p> <p>In the above example, we were sending MySQL logs directly to Elasticsearch, however, you can also send them to Logstash server. In Logstash, you can filter them, analyze, and eventually pass to Elasticsearch for storage. With Filebeat, this can be implemented with the following configuration.</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">filebeat.prospectors</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">input_type</span><span class="pi">:</span> <span class="s">log</span>
  <span class="na">paths</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">/var/log/mysql/*.log</span>
  <span class="na">document_type</span><span class="pi">:</span> <span class="s">syslog</span>
  <span class="na">registry</span><span class="pi">:</span> <span class="s">/var/lib/filebeat/registry</span>
<span class="na">output.logstash</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">mylogstashurl.example.com:5044"</span><span class="pi">]</span>
</code></pre></div></div> <h3 id="conclusion">Conclusion</h3> <p>As this tutorial demonstrates, Filebeat is an excellent log shipping solution for your MySQL database and Elasticsearch cluster. It’s extremely lightweight compared to its predecessors as it comes to efficiently sending log events. Filebeat supports compression and is easily configurable via a single <strong>yaml</strong> file. With Filebeat you can easily manage log files, keep track of log registry, create custom fields to enable granular filtering and discovery in your logs, and instantly power log data with Kibana visualization functionality.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="elasticsearch"/><category term="databases"/><summary type="html"><![CDATA[Effective log management involves a possibility to instantly draw useful insights from millions of log entries, identify issues as they arise, and visualize/communicate patterns that emerge out of your application logs. Fortunately, ELK stack (Elasticsearch, Logstash, and Kibana) makes it easy to ship logs from your application to ES collections for storage and analysis. Recently, Elastic infrastructure was extended by useful tools for shipping logs called Beats. Filebeat is a part of Beats tool set that can be configured to send log events either to Logstash (and from there to Elasticsearch), or even directly to the Elasticsearch. The tool turns your logs into searchable and filterable ES documents with fields and properties that can be easily visualized and analyzed.]]></summary></entry><entry><title type="html">How To Use Elasticsearch to Visualize Data?</title><link href="https://golkir.github.io/blog/2020/how_to_visualize_data_ES/" rel="alternate" type="text/html" title="How To Use Elasticsearch to Visualize Data?"/><published>2020-05-14T15:12:00+00:00</published><updated>2020-05-14T15:12:00+00:00</updated><id>https://golkir.github.io/blog/2020/how_to_visualize_data_ES</id><content type="html" xml:base="https://golkir.github.io/blog/2020/how_to_visualize_data_ES/"><![CDATA[<p>Elasticsearch mappings allow storing your data in formats which can be easily translated into meaningful visualizations capturing multiple complex relationships in your data. In this tutorial, we’ll show how to create data visualizations with Kibana, a part of ELK stack that makes it easy to search, view, and interact with data stored in Elasticsearch indices. We’ll walk you through basic data visualization types including line charts, area charts, pie charts, and time series after which you’ll be ready to design a custom visualization of any complexity.</p> <h2 id="get-data"><strong>Get Data</strong></h2> <p>For this tutorial, we’ll be using data supplied by <a href="https://www.elastic.co/products/beats/metricbeat">Metricbeat</a>, a light shipper that can be installed on your server to periodically collect metrics from the OS and various services running on the server. Metricbeat takes the metrics and sends them to the output you specify - in our case, a Qbox-hosted Elasticsearch cluster. Metricbeat currently supports system statistics and a wide variety of metrics from the popular software like MongoDB, Apache, Redis, MySQL, and many more. Data from these services includes diverse fields and parameters which makes Metricbeat a great tool for illustrating the power of Kibana data visualization.</p> <p>To start using Metricbeat data, you’ll need the following software installed and configured:</p> <ul> <li>Elasticsearch for storage and indexing of data. For this tutorial, we’re using a <a href="https://qbox.io/blog/provisioning-a-qbox-elasticsearch-cluster">Qbox-hosted Elasticsearch cluster.</a></li> <li>Kibana. A Qbox-hosted Elasticsearch ships with Kibana. When provisioning your cluster, just specify that you want to install it.</li> </ul> <p>To install Metricbeat with a deb package on the Linux system, run the following commands:</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-L</span> <span class="nt">-O</span> https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-6.2.3-amd64.deb
<span class="nb">sudo </span>dpkg <span class="nt">-i</span> metricbeat-6.2.3-amd64.deb
</code></pre></div></div> <p>Before using Metricbeat, we’ll need to configure the shipper in the <code class="language-plaintext highlighter-rouge">metricbeat.yml</code> file usually located in the<code class="language-plaintext highlighter-rouge">/etc/metricbeat/</code> folder on Linux distributions. In the configuration file, we at least need to specify Kibana’s and Elasticsearch’s hosts we want to send our data to and attach modules we want Metricbeat to collect data from. See <a href="https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-configuration.html">Metricbeat documentation</a> for more details about configuration.</p> <p>Once all configuration edits are made, we can start the Metricbeat service with the following command:</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>service metricbeat start
</code></pre></div></div> <p>Metricbeat will start periodically collecting and shipping data about your system and services to Elasticsearch. Now, we can use Kibana to display this data but before being able to do so, we have to add a <code class="language-plaintext highlighter-rouge">metricbeat-</code> index pattern to our Kibana management panel.</p> <p>After this is done, you’ll see the following index template with a list of fields sent by Metricbeat to your Qbox-hosted Elasticsearch instance.</p> <p><img src="" alt="Image1_metricbeat_index.png"/></p> <p>That’s it! We can now visualize our Metricbeat data using rich Kibana’s visualization features.</p> <h2 id="kibanas-visualize-module">Kibana’s Visualize Module</h2> <p>Kibana visualizations use Elasticsearch documents and their respective fields as inputs and Elasticsearch aggregations and metrics as utility functions to extract and process that data. Kibana supports numerous visualization types, including time series with Timelion and Visual Builder, various basic charts (e.g area charts, heat maps, horizontal bar charts, line charts and pie charts), tables, gauges, coordinate and region maps and tag clouds to name a few. All available visualization types can be accessed under <strong>Visualize</strong> section of the Kibana dashboard.</p> <p>The steps needed to create a visualization might differ depending on the visualization you want to produce, however, you should know basic definitions, metrics, and aggregations applied in most visualization types.</p> <p>The first step to create a standard Kibana visualization like a line- or bar chart is to select a metric which defines a value axis (usually a Y-axis). Kibana supports a number of Elasticsearch aggregations to represent your data in this axis:</p> <ul> <li> <p><strong>Count</strong></p> <p>Returns a count of the elements in the selected index pattern</p> </li> <li> <p><strong>Average</strong></p> <p>Returns the average of a numeric field selected in the drop-down.</p> </li> <li> <p><strong>Min</strong></p> <p>Returns the minimum value of a numeric field selected in the drop-down</p> </li> <li> <p><strong>Max</strong></p> <p>Returns the maximum value of a numeric field selected in the drop-down</p> </li> <li> <p><strong>Unique Count</strong></p> <p>This cardinality aggregation returns the number of unique values of a field</p> </li> <li> <p><strong>Standard Deviation</strong></p> <p>Returns the standard deviation of data in a numeric field.</p> </li> </ul> <p>These are just several parent aggregations available. For more metrics and aggregations consult <a href="https://www.elastic.co/guide/en/kibana/current/xy-chart.html">Kibana</a> documentation.</p> <p>Kibana also supports the bucket aggregations which determine what information to retrieve from your Elasticsearch index. This information is usually displayed above the X-axis of your chart which is normally the <em>buckets</em> axis. The X-axis supports the following aggregations for which you may find additional information in the main <a href="https://www.elastic.co/guide/en/kibana/current/xy-chart.html">Elasticsearch documentation</a>:</p> <ul> <li> <p><strong>Date Histogram</strong></p> <p>This aggregation is built from a numeric field and is organized by date. It allows specifying intervals for your historical data or design custom intervals.</p> </li> <li> <p><strong>Histogram</strong></p> <p>Builds a standard histogram from a numeric field. You need to specify an integer interval for this field.</p> </li> <li> <p><strong>Range</strong></p> <p>Range aggregation allows specifying ranges of values for a numeric field.</p> </li> <li> <p><strong>IPv4 Range</strong></p> <p>With this aggregation, you can specify ranges of IPv4 addresses.</p> </li> <li> <p><strong>Terms</strong></p> <p>With a terms aggregation, you can specify the top or bottom <em>n</em> elements of a field to display ordered by count or any other custom metric.</p> </li> <li> <p><strong>Filters</strong></p> <p>Kibana supports filters to specify rules for querying your Elasticsearch documents.</p> </li> </ul> <p>After we’ve specified aggregations for the X-axis, we can add sub-aggregations that refine the visualization. With this option, you can create charts with multiple buckets and aggregations of data. After all metrics and aggregations are defined, we can also customize the chart using custom labels, colors, and other useful features.</p> <h1 id="time-series-with-timelion">Time Series with Timelion</h1> <p><a href="https://www.elastic.co/blog/timelion-timeline">Timelion</a> is the time series composer for Kibana that enables to combine totally independent data sources in a single visualization using chainable functions. Timelion uses a simple expression language that allows retrieving time series data, making complex calculations and chaining additional visualizations.</p> <p>In the example below, we are combining a time series of the average CPU time spent in kernel space (<code class="language-plaintext highlighter-rouge">system.cpu.system.pct</code>) during the specified period of time with the same metric taken with a 20-minute offset. The expression below chains two <code class="language-plaintext highlighter-rouge">.es()</code> functions which define the ES index to retrieve data from, a time field to use for our time series, a field to apply our metric to (<code class="language-plaintext highlighter-rouge">system.cpu.system.pct</code>) and an offset value. Chaining these two functions allows visualizing dynamics of the CPU usage over time.</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.es<span class="o">(</span><span class="nv">index</span><span class="o">=</span>metricbeat-<span class="k">*</span>, <span class="nv">timefield</span><span class="o">=</span><span class="s1">'@timestamp'</span>, <span class="nv">metric</span><span class="o">=</span><span class="s1">'avg:system.cpu.system.pct'</span><span class="o">)</span>, .es<span class="o">(</span><span class="nv">offset</span><span class="o">=</span><span class="nt">-20m</span>,index<span class="o">=</span>metricbeat-<span class="k">*</span>, <span class="nv">timefield</span><span class="o">=</span><span class="s1">'@timestamp'</span>, <span class="nv">metric</span><span class="o">=</span><span class="s1">'avg:system.cpu.system.pct'</span><span class="o">)</span>
</code></pre></div></div> <p><img src="" alt="Image2_Timelion-Time-Series.png"/></p> <p>You can experiment with Timelion by doing similar comparisons for the percentage of the CPU time spent in user space, for low-priority processes, being idle and using numerous other metrics shipped by your Metricbeat instance.</p> <h2 id="visual-builder"><strong>Visual Builder</strong></h2> <p>A powerful alternative to Timelion for building time series visualization is the <a href="https://www.elastic.co/guide/en/kibana/current/time-series-visual-builder.html">Visual Builder</a> recently added to Kibana as a native module. Similarly to Timelion, Time Series Visual Builder enables to combine multiple aggregations and pipeline them to display complex data in a meaningful way. However, with Visual Builder, you can use simple UI to define metrics and aggregations instead of chaining functions manually as in Timelion.</p> <p>In the example below, we combine six time series that display the CPU usage in various spaces including user space, kernel space, CPU time spent on low-priority processes, time spent on handling hardware and software interrupts, and percentage of time spent in wait (on disc). To produce time series for each parameter, we define a metric that includes an aggregation type (e.g average) and the field name (e.g <code class="language-plaintext highlighter-rouge">system.cpu.user.pct</code>) for that parameter. For each metric, we can also specify a label to make our time series visualization more readable. With the Visual Builder, we can even create annotations that will attach additional data sources like system messages emitted at specific intervals to our Time Series visualization. It’s as easy as that!</p> <p><img src="" alt="Image3_Visualbuilder-time-series.png"/></p> <p>In addition to time series visualizations, Visual Builder supports other visualization types such as Metric, Top N, Gauge and Markdown which automatically convert our data into their respective visualization formats. For example, in the image below we’ve created a Top N simple visualization that displays top spaces where our CPU is used.</p> <p><img src="" alt="Image4_visual-builder-top-n.png"/></p> <p>In sum, Visual Builder is a great sandbox for experimentation with your data with which you can produce wonderful time series, gauges, metrics, and Top N lists.</p> <h3 id="creating-a-line-chart">Creating a Line Chart</h3> <p>A line chart is a basic type of chart that represents data as a series of data points connected by straight line segments. In the image below, you can see a line chart of the system load over the past 15 minutes. To create this chart, in the Y-axis, we used an average aggregation for the <code class="language-plaintext highlighter-rouge">system.load.1</code> field that calculates the system load average. You are not limited to the average aggregation, however, since Kibana supports a number of other Elasticsearch aggregations including median, standard deviation, min, max, and percentiles to name a few. You can play with them to figure out whether they work fine with the data you want to visualize.</p> <p>After defining the metric for the Y-axis, we specify parameters for our X-axis. In this example, we use data histogram for aggregation and the default <code class="language-plaintext highlighter-rouge">@timestamp</code> field to take timestamps from. After our parameters are entered, we can click on the ‘play’ button which will generate the line chart visualization with all axes and labels automatically added. That’s it! Now we can save our line chart to the dashboard by clicking ‘Save’ link in the top menu.</p> <p><img src="" alt="Image5_System-load-line-chart.png"/></p> <h3 id="pie-chart"><strong>Pie Chart</strong></h3> <p>A pie chart or a circle chart is a visualization type which is divided into different slices to illustrate numerical proportion.</p> <p>In this example, we’ll be using a split slice chart to visualize the CPU time usage by the processes running on our system. The first step to create our pie chart is to select a metric that defines how a slice’s size is determined. Kibana pie chart visualizations provide with three options for this metric: <em>count, sum, and unique count aggregations</em> discussed above. For our goal, we are interested in the <em>sum</em> aggregation for the<code class="language-plaintext highlighter-rouge">system.process.cpu.total.pct</code> field that describes the percentage of CPU time spent by the process since the last update. Once the metric is specified, we can also create a custom label for this value (e.g Total CPU usage by the process).</p> <p>The next step is to define our <em>buckets</em>. We’ve decided to use split slices chart which is a convenient way to visualize how parts make up the meaningful whole. For our buckets, we need to select a <em>Terms</em> aggregation that specifies the top or bottom <em>n</em> elements of a given field to display ordered by some metric. In our case, we’ll display 7 top processes running on our system ( <code class="language-plaintext highlighter-rouge">system.process.name</code> field) in terms of CPU time usage. The metric used to display our <em>Terms</em> aggregation will be the sum of the total CPU time usage by an individual process defined above. That’s it! Now, as always, let’s click play to see our pie chart.</p> <p><img src="" alt="Image6_pie-chart-processes.png"/></p> <p>As you see, Kibana automatically produced seven slices for the top seven processes in terms of CPU time usage. The size of each slice represents this value which is the highest for <code class="language-plaintext highlighter-rouge">supergiant</code> and <code class="language-plaintext highlighter-rouge">chrome</code> processes in our case. We can now save the created pie chart to the dashboard visualizations for later access.</p> <p><strong>Notice</strong>: when creating pie charts, remember that pie slices should sum up to a meaningful whole. In our case, this rule is followed - the whole is a sum of the CPU time usage by top seven processes running our system.</p> <h3 id="making-an-area-chart">Making an Area Chart</h3> <p>Area charts are just like line charts in that they represent the change in one or more quantities over time. The difference is, however, that area charts have the area between the X-axis and the line filled with color or shading.</p> <p>In Kibana, the area chart’s Y-axis is the <em>metrics</em> axis. It supports a number of aggregation types like count, average, sum, min, max, percentile, and more. In the example below, we drew an area chart that displays the percentage of CPU time usage by individual processes running on our system.</p> <p><img src="" alt="Image7_Area-chart-processes"/></p> <p>For the Y-Axis the metrics defined is the average for the field <code class="language-plaintext highlighter-rouge">system.process.cpu.total.pct</code> which can be higher than 100 percent if your computer has a multi-core processor. The next step is to specify the X-Axis metric and create individual buckets. In the X-Axis, we are using Date Histogram aggregation for the <code class="language-plaintext highlighter-rouge">@timestamp</code> field with the auto interval which defaults to 30 seconds. As an option, you can also select intervals ranging from milliseconds to years or even design your own interval.</p> <p>Once we’ve specified the Y-axis and X-axis aggregations, we can now define sub-aggregations to refine the visualization. For this example, we’ve selected <em>split series</em> which is a convenient way to represent the quantity change over time. Now, in order to represent the individual process, we define the ‘Terms’ sub-aggregation on the field <code class="language-plaintext highlighter-rouge">system.process.name</code> ordered by the previously defined CPU usage metric. In this bucket, we can also select the number of processes to display. That’s it! Now we can save our area chart visualization of the CPU usage by an individual process to the dashboard.</p> <h3 id="conclusion">Conclusion</h3> <p>Elasticsearch powered by Kibana makes data visualizations an extremely fun thing to do. Recent Kibana versions ship with a number of convenient templates and visualization types as well as a native Visualization Builder. With these features, you can construct anything ranging from a line chart to tag clouds leveraging Elasticsearch’s rich aggregation types and metrics. In the next tutorials, we will discuss more visualization options in Kibana such as coordinate and region maps and tag clouds.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="elasticsearch"/><category term="data-visualization"/><summary type="html"><![CDATA[Elasticsearch mappings allow storing your data in formats which can be easily translated into meaningful visualizations capturing multiple complex relationships in your data. In this tutorial, we’ll show how to create data visualizations with Kibana, a part of ELK stack that makes it easy to search, view, and interact with data stored in Elasticsearch indices. We’ll walk you through basic data visualization types including line charts, area charts, pie charts, and time series after which you’ll be ready to design a custom visualization of any complexity.]]></summary></entry><entry><title type="html">Integrating Elasticsearch Into Your Node.js Application</title><link href="https://golkir.github.io/blog/2020/elasticnodejs/" rel="alternate" type="text/html" title="Integrating Elasticsearch Into Your Node.js Application"/><published>2020-04-12T15:12:00+00:00</published><updated>2020-04-12T15:12:00+00:00</updated><id>https://golkir.github.io/blog/2020/elasticnodejs</id><content type="html" xml:base="https://golkir.github.io/blog/2020/elasticnodejs/"><![CDATA[<p>In previous tutorials, we have already discussed how to use Elasticsearch native clients in <a href="https://qbox.io/blog/elasticsearch-rest-client-idiomatic-rust-tutorial">Rust</a>, <a href="https://qbox.io/blog/rest-calls-new-java-elasticsearch-client-tutorial?utm_source=qbox.io&amp;utm_medium=article&amp;utm_campaign=elasticsearch-rest-client-idiomatic-rust-tutorial">Java</a>, and <a href="https://qbox.io/blog/python-scripts-interact-elasticsearch-examples?utm_source=qbox.io&amp;utm_medium=article&amp;utm_campaign=elasticsearch-rest-client-idiomatic-rust-tutorial">Python</a> among others. Today, we’re going to cover Node.js, - a popular JavaScript server-side solution based on event-driven architecture efficient for development of fast servers, I/O heavy applications, and real-time applications (RTAs). Node.js ecosystem is one of the fastest growing and it is now possible to interact with the Elasticsearch API using a Node.js client. <a href="https://github.com/elastic/elasticsearch-js">Elasticsearch.js</a> is the official Elasticsearch client for Node.js that ships with:</p> <ul> <li>one-to-one mapping with Elasticsearch REST API</li> <li>intelligent handling of node/connection failures</li> <li>support for load balancers</li> <li>integration into modern browsers</li> <li>efficient support for asynchronous operations with JavaScript Promises.</li> </ul> <p><a href="https://github.com/elastic/elasticsearch-js">Elasticsearch.js</a> has one of the widest support for standard and advanced Elasticsearch features and is regularly tested against ES releases 0.90.12 and greater. The client is regularly updated and maintained by the vibrant Node.js community.</p> <h2 id="tutorial"><strong>Tutorial</strong></h2> <p>For this tutorial, we will be using hosted Elasticsearch on <a href="https://qbox.io/">Qbox.io</a>. You can sign up and <a href="https://qbox.io/signup?utm_source=blog&amp;utm_campaign=tutorial&amp;utm_term=launch_your_cluster&amp;utm_medium=article">launch Elasticsearch cluster here</a>, or click “Get Started” in the header navigation menu. If you need help setting up, refer to “<a href="https://qbox.io/blog/provisioning-a-qbox-elasticsearch-cluster?utm_source=tutorial&amp;utm_term=provision&amp;utm_medium=article&amp;utm_campaign=index-attachments-files-elasticsearch-mapper">Provisioning a Qbox Elasticsearch Cluster</a>.” Before you proceed to the tutorial, ensure that the latest version of Node.js is installed on your computer. If you are not sure how to do this, please, refer to <a href="https://nodejs.org/uk/download/package-manager/">Node.js installation guide</a> for directions. Once we have Node.js up and running, we need to install Elasticsearch.js in our Node.js application. You can install the official elasticsearch.js module via npm (Node Package Manager) command:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>npm install elasticsearch
</code></pre></div></div> <p>Using the elasticsearch.js module in Node we can easily connect to and interact with our Elasticsearch cluster on Qbox.io:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">elasticsearch</span> <span class="o">=</span> <span class="nf">require</span><span class="p">(</span><span class="dl">'</span><span class="s1">elasticsearch</span><span class="dl">'</span><span class="p">);</span>
<span class="kd">var</span> <span class="nx">client</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">elasticsearch</span><span class="p">.</span><span class="nc">Client</span><span class="p">({</span>
   <span class="na">hosts</span><span class="p">:</span> <span class="p">[</span> <span class="dl">'</span><span class="s1">https://username:password@host:port</span><span class="dl">'</span><span class="p">]</span>
<span class="p">});</span>
</code></pre></div></div> <p>The client constructor accepts a config object/hash where you can define <a href="https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/configuration.html">defaults parameters</a>, or even entire classes, for the client to use. In addition, our client variable inherits all methods of the elasticsearch.js Client constructor which corresponds to the official Elasticsearch API.</p> <p>We can now ping to our Qbox.io cluster to check if everything is ok.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">client</span><span class="p">.</span><span class="nf">ping</span><span class="p">({</span>
     <span class="na">requestTimeout</span><span class="p">:</span> <span class="mi">30000</span><span class="p">,</span>
 <span class="p">},</span> <span class="kd">function</span><span class="p">(</span><span class="nx">error</span><span class="p">)</span> <span class="p">{</span>
     <span class="k">if </span><span class="p">(</span><span class="nx">error</span><span class="p">)</span> <span class="p">{</span>
         <span class="nx">console</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="dl">'</span><span class="s1">elasticsearch cluster is down!</span><span class="dl">'</span><span class="p">);</span>
     <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
         <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Everything is ok</span><span class="dl">'</span><span class="p">);</span>
     <span class="p">}</span>
 <span class="p">});</span>
</code></pre></div></div> <p>If something goes wrong, you can debug using the error object returned by the callback. Elasticsearch API also includes other useful methods to inspect your Qbox.io cluster. For example, using a cluster object you can check cluster health (<strong>cluster.health</strong>), stats, current state (e.g operational), and more.</p> <h2 id="features-of-elasticsearchjs"><strong>Features of Elasticsearch.js</strong></h2> <p>Now, we are going to discuss basic features of elasticsearch.js such as indexing, creating documents, and search.</p> <h2 id="indexing"><strong>Indexing</strong></h2> <p>Unlike conventional databases, an Elasticsearch index is a place to store related documents. In this example, we’re going to create an index ‘blog’ to store ‘posts’ in it. With elasticsearch.js it can be done as easy as that:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nx">client</span><span class="p">.</span><span class="nx">indices</span><span class="p">.</span><span class="nf">create</span><span class="p">({</span>
     <span class="na">index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">blog</span><span class="dl">'</span>
 <span class="p">},</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">resp</span><span class="p">,</span> <span class="nx">status</span><span class="p">)</span> <span class="p">{</span>
     <span class="k">if </span><span class="p">(</span><span class="nx">err</span><span class="p">)</span> <span class="p">{</span>
         <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="nx">err</span><span class="p">);</span>
     <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
         <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">create</span><span class="dl">"</span><span class="p">,</span> <span class="nx">resp</span><span class="p">);</span>
     <span class="p">}</span>
 <span class="p">});</span>
</code></pre></div></div> <p>If the index already exists, you get <strong>‘index_already_exists_exception’</strong>. Otherwise, a new index ready to store your documents is created.</p> <h2 id="adding-documents-to-an-index"><strong>Adding Documents to an Index</strong></h2> <p>Now as we’ve got an index, we need some documents to post to it. For that purpose, we are going to index a new document with a type ‘posts’.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nx">client</span><span class="p">.</span><span class="nf">index</span><span class="p">({</span>
     <span class="na">index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">blog</span><span class="dl">'</span><span class="p">,</span>
     <span class="na">id</span><span class="p">:</span> <span class="dl">'</span><span class="s1">1</span><span class="dl">'</span><span class="p">,</span>
     <span class="na">type</span><span class="p">:</span> <span class="dl">'</span><span class="s1">posts</span><span class="dl">'</span><span class="p">,</span>
     <span class="na">body</span><span class="p">:</span> <span class="p">{</span>
         <span class="dl">"</span><span class="s2">PostName</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Integrating Elasticsearch Into Your Node.js Application</span><span class="dl">"</span><span class="p">,</span>
         <span class="dl">"</span><span class="s2">PostType</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Tutorial</span><span class="dl">"</span><span class="p">,</span>
         <span class="dl">"</span><span class="s2">PostBody</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">This is the text of our tutorial about using Elasticsearch in your Node.js application.</span><span class="dl">"</span><span class="p">,</span>
     <span class="p">}</span>
 <span class="p">},</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">resp</span><span class="p">,</span> <span class="nx">status</span><span class="p">)</span> <span class="p">{</span>
     <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="nx">resp</span><span class="p">);</span>
 <span class="p">});</span>
</code></pre></div></div> <p>Here, the body is a typed JSON document in an index that makes it searchable. We have defined a body object with key parameters of our document: <strong>PostName, PostType, and PostBody</strong>. You can add whatever parameters of the blog post you need. The inner process of posting a new document works like this. If <strong>id</strong> param is not specified, Elasticsearch will auto-generate a unique <strong>id</strong>. In case you specify an <strong>id</strong>, Elasticsearch will either create a new document (if it does not exist) or update an existing one. Similarly, Elasticsearch will perform optimistic concurrency control when the version argument is used. If the document is successfully saved, we get the following response from the server:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span> <span class="nl">_index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">blog</span><span class="dl">'</span><span class="p">,</span>
  <span class="nx">_type</span><span class="p">:</span> <span class="dl">'</span><span class="s1">posts</span><span class="dl">'</span><span class="p">,</span>
  <span class="nx">_id</span><span class="p">:</span> <span class="dl">'</span><span class="s1">1</span><span class="dl">'</span><span class="p">,</span>
  <span class="nx">_version</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="nx">result</span><span class="p">:</span> <span class="dl">'</span><span class="s1">created</span><span class="dl">'</span><span class="p">,</span>
  <span class="nx">_shards</span><span class="p">:</span> <span class="p">{</span> <span class="nl">total</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">successful</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">failed</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span>
  <span class="nx">created</span><span class="p">:</span> <span class="kc">true</span> <span class="p">}</span>
</code></pre></div></div> <p>If the index exists and we didn’t specify a version argument, the existing document will be simply updated by the new one and specified as its second version.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span> <span class="nl">_index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">blog</span><span class="dl">'</span><span class="p">,</span>
  <span class="nx">_type</span><span class="p">:</span> <span class="dl">'</span><span class="s1">posts</span><span class="dl">'</span><span class="p">,</span>
  <span class="nx">_id</span><span class="p">:</span> <span class="dl">'</span><span class="s1">1</span><span class="dl">'</span><span class="p">,</span>
  <span class="nx">_version</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
  <span class="nx">result</span><span class="p">:</span> <span class="dl">'</span><span class="s1">updated</span><span class="dl">'</span><span class="p">,</span>
  <span class="nx">_shards</span><span class="p">:</span> <span class="p">{</span> <span class="nl">total</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">successful</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">failed</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span>
  <span class="nx">created</span><span class="p">:</span> <span class="kc">false</span> <span class="p">}</span>
</code></pre></div></div> <p>In addition to creating documents, elasticsearch.js supports index flushing, analysis, recovery, creation of mappings, simulation, and other advanced methods.</p> <p>**</p> <h2 id="search-documents-using-query-params"><strong>Search Documents Using Query Params</strong></h2> <p>** Elasticsearch.js has a mature search functionality that supports both simple queries and Elasticsearch Query DSL. To search documents using simple query you need to specify a <strong>‘q’</strong> parameter in your request object. In this example, we are searching for all posts with “Node.js” in the post title.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">client</span><span class="p">.</span><span class="nf">search</span><span class="p">({</span>
    <span class="na">index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">blog</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">type</span><span class="p">:</span> <span class="dl">'</span><span class="s1">posts</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">q</span><span class="p">:</span> <span class="dl">'</span><span class="s1">PostName:Node.js</span><span class="dl">'</span>
<span class="p">}).</span><span class="nf">then</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">resp</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="nx">resp</span><span class="p">);</span>
<span class="p">},</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nf">trace</span><span class="p">(</span><span class="nx">err</span><span class="p">.</span><span class="nx">message</span><span class="p">);</span>
<span class="p">});</span>
</code></pre></div></div> <p>**</p> <h2 id="elasticsearch-query-dsl"><strong>Elasticsearch Query DSL</strong></h2> <p>** If you want to have a fine-grained control over document search, Elasticsearch offers a query DSL.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">client</span><span class="p">.</span><span class="nf">search</span><span class="p">({</span>
    <span class="na">index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">blog</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">type</span><span class="p">:</span> <span class="dl">'</span><span class="s1">posts</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">body</span><span class="p">:</span> <span class="p">{</span>
        <span class="na">query</span><span class="p">:</span> <span class="p">{</span>
            <span class="na">match</span><span class="p">:</span> <span class="p">{</span>
                <span class="dl">"</span><span class="s2">PostName</span><span class="dl">"</span><span class="p">:</span> <span class="dl">'</span><span class="s1">Node.js</span><span class="dl">'</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}).</span><span class="nf">then</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">resp</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="nx">resp</span><span class="p">);</span>
<span class="p">},</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nf">trace</span><span class="p">(</span><span class="nx">err</span><span class="p">.</span><span class="nx">message</span><span class="p">);</span>
<span class="p">});</span>
</code></pre></div></div> <p>A successful response looks like this:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span> <span class="nl">took</span><span class="p">:</span> <span class="mi">407</span><span class="p">,</span>
  <span class="nx">timed_out</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nx">_shards</span><span class="p">:</span> <span class="p">{</span> <span class="nl">total</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="nx">successful</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="nx">failed</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span>
  <span class="nx">hits</span><span class="p">:</span> <span class="p">{</span> <span class="nl">total</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">max_score</span><span class="p">:</span> <span class="mf">0.26742277</span><span class="p">,</span> <span class="nx">hits</span><span class="p">:</span> <span class="p">[</span> <span class="p">[</span><span class="nb">Object</span><span class="p">]</span> <span class="p">]</span> <span class="p">}</span> <span class="p">}</span>
</code></pre></div></div> <p>It shows that a total number of documents with the title that matches the query (‘Node.js’) is equal to one.</p> <p>If no documents match the query, Qbox ElasticSearch returns:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span> <span class="nl">took</span><span class="p">:</span> <span class="mi">69</span><span class="p">,</span>
  <span class="nx">timed_out</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nx">_shards</span><span class="p">:</span> <span class="p">{</span> <span class="nl">total</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="nx">successful</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="nx">failed</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span>
  <span class="nx">hits</span><span class="p">:</span> <span class="p">{</span> <span class="nl">total</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="nx">max_score</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span> <span class="nx">hits</span><span class="p">:</span> <span class="p">[]</span> <span class="p">}</span> <span class="p">}</span>
</code></pre></div></div> <p>Elasticsearch.js supports all Query DSL features including leaf clauses and compound clauses. As an added bonus, you can use wildcard searches and regular expressions. In the example below, we create a query for all matches where ‘.js’ is preceded by four characters.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">query</span><span class="p">:</span> <span class="p">{</span> <span class="nl">wildcard</span><span class="p">:</span> <span class="p">{</span> <span class="dl">"</span><span class="s2">PostBody</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">????.js</span><span class="dl">"</span> <span class="p">}</span> <span class="p">}</span>
</code></pre></div></div> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>Elasticsearch.js is a very mature Elasticsearch client for Node.js able to handle basic use cases and supporting many advanced ones. In addition to the aforementioned functionality, elasticsearch.js supports searching of shards, scrolling, bulk operations in a single API call and more. Broad coverage of low-level Elasticsearch functions and leveraging the power of Javascript Promises makes this library the best choice for your Node.js application.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="elasticsearch"/><summary type="html"><![CDATA[In previous tutorials, we have already discussed how to use Elasticsearch native clients in Rust, Java, and Python among others. Today, we’re going to cover Node.js, - a popular JavaScript server-side solution based on event-driven architecture efficient for development of fast servers, I/O heavy applications, and real-time applications (RTAs). Node.js ecosystem is one of the fastest growing and it is now possible to interact with the Elasticsearch API using a Node.js client. Elasticsearch.js is the official Elasticsearch client for Node.js that ships with:]]></summary></entry><entry><title type="html">Using Advanced Elasticsearch Methods With Node.js Elasticsearch Client</title><link href="https://golkir.github.io/blog/2019/elasticsearch_advanced_tutorial/" rel="alternate" type="text/html" title="Using Advanced Elasticsearch Methods With Node.js Elasticsearch Client"/><published>2019-12-12T00:00:00+00:00</published><updated>2019-12-12T00:00:00+00:00</updated><id>https://golkir.github.io/blog/2019/elasticsearch_advanced_tutorial</id><content type="html" xml:base="https://golkir.github.io/blog/2019/elasticsearch_advanced_tutorial/"><![CDATA[<p>In the previous <a href="https://qbox.io/blog/integrating-elasticsearch-into-node-js-application">tutorial</a>, we have discussed how to use elasticsearch.js, the official Node.js client for Elasticsearch, to index, add documents, and search them using simple queries and Query DSL. In this tutorial, we’re going to dive deeper into elasticsearch.js describing more advanced methods and concepts like scrolling, aggregations, and analyzers.</p> <p>As always, we will be using hosted Elasticsearch on Qbox.io. We assume that you have installed the latest version of Node.js, downloaded the elasticsearch.js module into your Node.js application and connected it to your Elasticsearch cluster as described in the previous tutorial.</p> <h3 id="scrolling-">**Scrolling **</h3> <p>Search requests discussed in the <a href="https://qbox.io/blog/integrating-elasticsearch-into-node-js-application">previous tutorial</a> are convenient if you want to retrieve a single ‘<em>page</em>’ of results. However, what if you need to return a large number of results from a single request? Then, you would probably need a cursor implemented in the Elasticsearch as a Scroll API. Keep in mind that scrolling is not intended for the real-time user requests (normally, you would paginate the content for users), but rather for batch processing of a large amount of data or re-indexing your content.</p> <p>Below is an example of how we can use scrolling to retrieve all blog posts featuring ‘Node’ in their title and save them to the <strong>allTitles</strong> array for later processing.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">allTitles</span> <span class="o">=</span> <span class="p">[];</span>
<span class="nx">client</span><span class="p">.</span><span class="nf">search</span><span class="p">({</span>
            <span class="na">index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">posts</span><span class="dl">'</span><span class="p">,</span>
            <span class="na">scroll</span><span class="p">:</span> <span class="dl">'</span><span class="s1">10s</span><span class="dl">'</span><span class="p">,</span> <span class="c1">// keep the search results "scrollable" for 10 seconds</span>
            <span class="na">source</span><span class="p">:</span> <span class="p">[</span><span class="dl">'</span><span class="s1">title</span><span class="dl">'</span><span class="p">],</span> <span class="c1">// filter the source to only include the title field</span>
            <span class="na">q</span><span class="p">:</span> <span class="dl">'</span><span class="s1">title:Node</span><span class="dl">'</span>
        <span class="p">},</span> <span class="kd">function</span> <span class="nf">getMoreUntilDone</span><span class="p">(</span><span class="nx">error</span><span class="p">,</span> <span class="nx">response</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// collect the title from each response</span>
            <span class="nx">response</span><span class="p">.</span><span class="nx">hits</span><span class="p">.</span><span class="nx">hits</span><span class="p">.</span><span class="nf">forEach</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">hit</span><span class="p">)</span> <span class="p">{</span>
                <span class="nx">allTitles</span><span class="p">.</span><span class="nf">push</span><span class="p">(</span><span class="nx">hit</span><span class="p">.</span><span class="nx">_source</span><span class="p">.</span><span class="nx">title</span><span class="p">);</span>
            <span class="p">});</span>
</code></pre></div></div> <p>Here, we are using a scroll parameter with a value <strong>‘10s’</strong>, which tells Elasticsearch how long it should keep the “search context”. This value should not be big enough because it is used only to process the previous batch of results, not the entire set of results. Also, the callback function <strong>getMoreUntilDone</strong> returns a response object with <strong>_scroll_id</strong> which indicates the id of a current scroll. This id should then be passed to the Scroll API in order to retrieve the next batch of results. The <strong>_scroll_id</strong> looks like this:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">POST</span> <span class="o">/</span><span class="nx">_search</span><span class="o">/</span><span class="nx">scroll</span> 
<span class="p">{</span>
    <span class="dl">"</span><span class="s2">scroll</span><span class="dl">"</span> <span class="p">:</span> <span class="dl">"</span><span class="s2">10s</span><span class="dl">"</span><span class="p">,</span> 
    <span class="dl">"</span><span class="s2">scroll_id</span><span class="dl">"</span> <span class="p">:</span> <span class="dl">"</span><span class="s2">DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==</span><span class="dl">"</span> 
<span class="p">}</span>
</code></pre></div></div> <p>After we get <strong>_scroll_id</strong>, we can continue with retrieving our posts.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if </span><span class="p">(</span><span class="nx">response</span><span class="p">.</span><span class="nx">hits</span><span class="p">.</span><span class="nx">total</span> <span class="o">&gt;</span> <span class="nx">allTitles</span><span class="p">.</span><span class="nx">length</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">//ask elasticsearch for the next set of hits from this search</span>
    <span class="nx">client</span><span class="p">.</span><span class="nf">scroll</span><span class="p">({</span>
        <span class="na">scrollId</span><span class="p">:</span> <span class="nx">response</span><span class="p">.</span><span class="nx">_scroll_id</span><span class="p">,</span>
        <span class="na">scroll</span><span class="p">:</span> <span class="dl">'</span><span class="s1">10s</span><span class="dl">'</span>
    <span class="p">},</span> <span class="nx">getMoreUntilDone</span><span class="p">);</span>
<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">every Node title</span><span class="dl">'</span><span class="p">,</span> <span class="nx">allTitles</span><span class="p">);</span>
<span class="p">}</span>
<span class="p">});</span>
</code></pre></div></div> <p>What this code actually does is specifying a scroll id for the next batch of pasts (and because <strong>_scroll_id</strong> changes with each request, we need to specify the newest one) and recursively calling <strong>getMoreUntilDone</strong> function as long as the total number of hits in our response is greater than the length of the **allTitles ** array.</p> <h3 id="aggregations">Aggregations</h3> <p>Aggregations are useful whenever you want to build analytic and quantitative information over a set of documents. Elasticsearch.js supports key types of Elasticsearch aggregations including metrics aggregations for computing numerical parameters of your documents (e.g average value, max/min value or percentile ranks over a set of documents), bucketing aggregations for defining sets of documents associated with certain criterion, matrix aggregations operating on multiple fields and producing matrix result based on values extracted from those, and pipeline aggregations that aggregate the output of other aggregations and their associated metrics.</p> <p>To understand how aggregations work in elasticsearch.js, let’s address the case of metrics aggregations. Assuming we have an index named <strong>‘grades’</strong> where we store student grades in specific subjects (e.g math), our task is to calculate the average grade and max/min grades earned by students in our collection. To accomplish this, we first need to define the aggregation parameters in our search query.</p> <p>A typical structure of aggregations definition looks like this:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="dl">"</span><span class="s2">aggregations</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span>
    <span class="dl">"</span><span class="s2">&lt;aggregation_name&gt;</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span>
        <span class="dl">"</span><span class="s2">&lt;aggregation_type&gt;</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span>
            <span class="o">&lt;</span><span class="nx">aggregation_body</span><span class="o">&gt;</span>
        <span class="p">}</span>
        <span class="p">[,</span><span class="dl">"</span><span class="s2">meta</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span>  <span class="p">[</span><span class="o">&lt;</span><span class="nx">meta_data_body</span><span class="o">&gt;</span><span class="p">]</span> <span class="p">}</span> <span class="p">]?</span>
        <span class="p">[,</span><span class="dl">"</span><span class="s2">aggregations</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span> <span class="p">[</span><span class="o">&lt;</span><span class="nx">sub_aggregation</span><span class="o">&gt;</span><span class="p">]</span><span class="o">+</span> <span class="p">}</span> <span class="p">]?</span>
    <span class="p">}</span>
    <span class="p">[,</span><span class="dl">"</span><span class="s2">&lt;aggregation_name_2&gt;</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span> <span class="p">...</span> <span class="p">}</span> <span class="p">]</span><span class="o">*</span>
<span class="p">}</span>
</code></pre></div></div> <p>Using elasticsearch.js, we can define aggregations as the object within search request body:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">client</span><span class="p">.</span><span class="nf">search</span><span class="p">({</span>
    <span class="na">index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">grades</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">body</span><span class="p">:</span> <span class="p">{</span>
        <span class="dl">"</span><span class="s2">aggs</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="dl">"</span><span class="s2">avg_grade</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
                <span class="dl">"</span><span class="s2">avg</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
                    <span class="dl">"</span><span class="s2">field</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">grade</span><span class="dl">"</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}).</span><span class="nf">then</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">resp</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="nx">resp</span><span class="p">);</span>
<span class="p">},</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nf">trace</span><span class="p">(</span><span class="nx">err</span><span class="p">.</span><span class="nx">message</span><span class="p">);</span>
<span class="p">});</span>
</code></pre></div></div> <p>In this code, <strong>avg_grade</strong> is a variable we define to store the result of our average grade computation and a ‘field’ property specifies the field that should be aggregated. In our case, this is a ‘grade’ field that stores student grades.</p> <p>If the request is successful, we get a response that looks like this:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">aggregations</span><span class="p">:{</span> <span class="nl">avg_grade</span><span class="p">:</span> <span class="p">{</span> <span class="na">value</span><span class="p">:</span> <span class="mf">70.33333333333333</span> <span class="p">}</span> <span class="p">}</span> <span class="p">}</span>
</code></pre></div></div> <p>We can include as many aggregation types in our <strong>aggs</strong> object as we want. For example, in the code snippet below we simultaneously calculate the average, min and max values of our grades.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">client</span><span class="p">.</span><span class="nx">search</span><span class="p">({</span>
            <span class="na">index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">grades</span><span class="dl">'</span><span class="p">,</span>
            <span class="na">body</span><span class="p">:</span> <span class="p">{</span>
                <span class="dl">"</span><span class="s2">aggs</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
                    <span class="dl">"</span><span class="s2">avg_grade</span><span class="dl">"</span><span class="p">:</span>
                    <span class="p">{</span>
                        <span class="dl">"</span><span class="s2">avg</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
                            <span class="dl">"</span><span class="s2">field</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">grade</span><span class="dl">"</span>
                        <span class="p">}</span>
                    <span class="p">},</span>
                    <span class="dl">"</span><span class="s2">min_grade</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
                        <span class="dl">"</span><span class="s2">min</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
                            <span class="dl">"</span><span class="s2">field</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">grade</span><span class="dl">"</span>
                        <span class="p">}</span>
                    <span class="p">},</span>
                    <span class="dl">"</span><span class="s2">max_grade</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
                        <span class="dl">"</span><span class="s2">max</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
                            <span class="dl">"</span><span class="s2">field</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">grade</span><span class="dl">"</span>
                        <span class="p">}</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>
</code></pre></div></div> <p>If the response is successful, we get the following results:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="p">{</span> <span class="nl">max_grade</span><span class="p">:</span> <span class="p">{</span> <span class="na">value</span><span class="p">:</span> <span class="mi">94</span> <span class="p">},</span>
    <span class="nx">min_grade</span><span class="p">:{</span> <span class="nl">value</span><span class="p">:</span> <span class="mi">43</span> <span class="p">},</span>
    <span class="nx">avg_grade</span><span class="p">:{</span> <span class="nl">value</span><span class="p">:</span> <span class="mf">70.33333333333333</span> <span class="p">}</span> <span class="p">}</span> <span class="p">}</span>
</code></pre></div></div> <p>We can also use cardinality aggregation to calculate a number of unique entities in our collection. For example, if you are indexing musical albums you can count unique bands that match the user query.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
    <span class="dl">"</span><span class="s2">aggs</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span>
        <span class="dl">"</span><span class="s2">band_count</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span>
            <span class="dl">"</span><span class="s2">cardinality</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span>
                <span class="dl">"</span><span class="s2">field</span><span class="dl">"</span> <span class="p">:</span> <span class="dl">"</span><span class="s2">band</span><span class="dl">"</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>The successful response below means that there are 3 unique bands in our albums collection.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
    <span class="dl">"</span><span class="s2">aggregations</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span>
        <span class="dl">"</span><span class="s2">type_count</span><span class="dl">"</span> <span class="p">:</span> <span class="p">{</span>
            <span class="dl">"</span><span class="s2">value</span><span class="dl">"</span> <span class="p">:</span> <span class="mi">3</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>In addition, Metrics aggregations support Geo points and their centroid calculation, percentiles, the sum of numeric values extracted from aggregated documents and other useful methods.</p> <h3 id="language-analyzers">Language Analyzers</h3> <p>Language analyzers convert text, like a musical album’s title into tokens or terms which are added to the inverted index for searching. This index will include not only exact words but plural word forms, infinitive verbs etc. This conversion makes it easy to match user queries (which might not be exactly correct) with documents in your collection.</p> <p>For example, the in-built <em>english</em> analyzer will transform</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>“the QUICK brown foxes jumped over the lazy dog!”
</code></pre></div></div> <p>into the following inverted index:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[quick, brown, fox, jump, over, lazy, dog]
</code></pre></div></div> <p>To set a language analyzer to <em>english</em> we should specify a mapping for a searchable property of our document. Let’s assume that we have created an index named “albums” to store musical albums. Then, we can insert a new mapping with <em>english</em> analyzer for a property “title” of our document “album”. This makes our album titles searchable according to the <em>english</em> analyzer rules mentioned above.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">client</span><span class="p">.</span><span class="nx">indices</span><span class="p">.</span><span class="nf">putMapping</span><span class="p">({</span>
    <span class="na">index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">albums</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">type</span><span class="p">:</span> <span class="dl">'</span><span class="s1">album</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">body</span><span class="p">:</span> <span class="p">{</span>
        <span class="na">properties</span><span class="p">:</span> <span class="p">{</span>
            <span class="dl">'</span><span class="s1">title</span><span class="dl">'</span><span class="p">:</span> <span class="p">{</span>
                <span class="dl">'</span><span class="s1">type</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">string</span><span class="dl">'</span><span class="p">,</span> <span class="c1">// type is a required attribute if index is specified</span>
                <span class="dl">'</span><span class="s1">analyzer</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">english</span><span class="dl">'</span>
            <span class="p">},</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">},</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">resp</span><span class="p">,</span> <span class="nx">status</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if </span><span class="p">(</span><span class="nx">err</span><span class="p">)</span> <span class="p">{</span>
        <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="nx">err</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="nx">resp</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">});</span>
</code></pre></div></div> <p>Now, if we have a Pink Floyd’s 1973 album “The Dark Side of the Moon” in our album list, users can find it using the following query.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">client</span><span class="p">.</span><span class="nf">search</span><span class="p">({</span>
    <span class="na">index</span><span class="p">:</span> <span class="dl">'</span><span class="s1">albums</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">type</span><span class="p">:</span> <span class="dl">'</span><span class="s1">album</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">q</span><span class="p">:</span> <span class="dl">"</span><span class="s2">title:moons</span><span class="dl">"</span>
<span class="p">}).</span><span class="nf">then</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">resp</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="nx">resp</span><span class="p">);</span>
<span class="p">},</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nf">trace</span><span class="p">(</span><span class="nx">err</span><span class="p">.</span><span class="nx">message</span><span class="p">);</span>
<span class="p">});</span>
</code></pre></div></div> <p>Even though the exact word “<strong>moons</strong>” used in the query string does not appear in the album’s title (it reads “The Dark side of the <strong>moon</strong>”), since we have applied the same analyzer both to the ‘title’ property and the query string, the query matches an inverted index and returns Pink Floyd’s album successfully.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hits:{ total: 1, max_score: 0.25316024, hits: [ [Object] ] } }
</code></pre></div></div> <h3 id="conclusion">Conclusion</h3> <p>Hopefully, you’ve got some intuition about using advanced Elasticsearch methods with Node.js elasticsearch.js client. Elasticsearch.js is a very powerful tool that provides one-to-one mapping with Elasticsearch REST API which means it covers the vast majority of methods used in the native Elasticsearch. Broad coverage of low-level Elasticsearch features, support for load balancers, intelligent handling of node/connection failures and easy integration into modern browsers makes elasticsearch.js the best solution for your Node.js applications.</p>]]></content><author><name>Kirill Goltsman</name></author><category term="elasticsearch"/><category term="nodejs"/><summary type="html"><![CDATA[In the previous tutorial, we have discussed how to use elasticsearch.js, the official Node.js client for Elasticsearch, to index, add documents, and search them using simple queries and Query DSL. In this tutorial, we’re going to dive deeper into elasticsearch.js describing more advanced methods and concepts like scrolling, aggregations, and analyzers.]]></summary></entry></feed>